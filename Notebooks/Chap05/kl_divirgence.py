"""
Модуль для вычисления дивергенции Кульбака-Лейблера (KL-divergence),
энтропии и кросс-энтропии для дискретных распределений.
"""

import numpy as np

def kl_divergence(p, q, eps=1e-12):
    """
    Вычисляет дивергенцию Кульбака-Лейблера KL(P||Q) для дискретных распределений.
    
    Дивергенция Кульбака-Лейблера измеряет различие между двумя распределениями
    вероятностей P и Q. Это асимметричная мера: KL(P||Q) != KL(Q||P).
    
    Формула: KL(P||Q) = Σ p(x) * log(p(x) / q(x))
    
    Параметры:
    ----------
    p : array-like
        Истинное распределение вероятностей P
    q : array-like
        Приближенное распределение вероятностей Q
    eps : float, optional (по умолчанию 1e-12)
        Малое значение для предотвращения деления на 0 и вычисления log(0)
        
    Возвращает:
    -----------
    float
        Значение дивергенции Кульбака-Лейблера KL(P||Q)
        
    Примеры:
    --------
    >>> P = [0.7, 0.2, 0.1]
    >>> Q = [0.33, 0.33, 0.34]
    >>> kl_divergence(P, Q)
    0.589...
    """
    # Преобразуем входные данные в numpy массивы типа float
    p = np.array(p, dtype=float)
    q = np.array(q, dtype=float)

    # Нормализуем распределения, чтобы сумма вероятностей была равна 1
    p = p / p.sum()
    q = q / q.sum()

    # Обрезаем значения снизу до eps, чтобы избежать проблем с log(0) и делением на 0
    p = np.clip(p, eps, 1.0)
    q = np.clip(q, eps, 1.0)

    # Вычисляем KL-дивергенцию: Σ p(x) * log(p(x) / q(x))
    return np.sum(p * np.log(p / q))


def entropy(p, eps=1e-12):
    """
    Вычисляет энтропию Шеннона для дискретного распределения вероятностей.
    
    Энтропия измеряет среднее количество информации, содержащееся в событии
    из распределения P. Чем выше энтропия, тем более неопределенным является распределение.
    
    Формула: H(P) = -Σ p(x) * log(p(x))
    
    Параметры:
    ----------
    p : array-like
        Распределение вероятностей
    eps : float, optional (по умолчанию 1e-12)
        Малое значение для предотвращения вычисления log(0)
        
    Возвращает:
    -----------
    float
        Значение энтропии распределения P
        
    Примеры:
    --------
    >>> P = [0.7, 0.2, 0.1]
    >>> entropy(P)
    0.801...
    """
    # Преобразуем входные данные в numpy массив типа float
    p = np.array(p, dtype=float)
    
    # Нормализуем распределение
    p = p / p.sum()
    
    # Обрезаем значения снизу до eps, чтобы избежать log(0)
    p = np.clip(p, eps, 1.0)
    
    # Вычисляем энтропию: -Σ p(x) * log(p(x))
    return -np.sum(p * np.log(p))

def cross_entropy(p, q, eps=1e-12):
    """
    Вычисляет кросс-энтропию между двумя дискретными распределениями.
    
    Кросс-энтропия измеряет среднее количество бит, необходимое для кодирования
    событий из распределения P при использовании кода, оптимизированного для Q.
    
    Формула: H(P, Q) = -Σ p(x) * log(q(x))
    
    Кросс-энтропия связана с энтропией и KL-дивергенцией:
    H(P, Q) = H(P) + KL(P||Q)
    
    Параметры:
    ----------
    p : array-like
        Истинное распределение вероятностей P
    q : array-like
        Предполагаемое распределение вероятностей Q
    eps : float, optional (по умолчанию 1e-12)
        Малое значение для предотвращения вычисления log(0)
        
    Возвращает:
    -----------
    float
        Значение кросс-энтропии H(P, Q)
        
    Примеры:
    --------
    >>> P = [0.7, 0.2, 0.1]
    >>> Q = [0.33, 0.33, 0.34]
    >>> cross_entropy(P, Q)
    1.391...
    """
    # Преобразуем входные данные в numpy массивы типа float
    p = np.array(p, dtype=float)
    q = np.array(q, dtype=float)
    
    # Нормализуем оба распределения
    p = p / p.sum()
    q = q / q.sum()
    
    # Обрезаем значения снизу до eps, чтобы избежать log(0)
    p = np.clip(p, eps, 1.0)
    q = np.clip(q, eps, 1.0)
    
    # Вычисляем кросс-энтропию: -Σ p(x) * log(q(x))
    return -np.sum(p * np.log(q))


# ============================================================================
# Пример использования функций
# ============================================================================

# Задаем истинное распределение P и его приближение Q
P = [0.7, 0.2, 0.1]  # Истинное распределение вероятностей
Q = [0.33, 0.33, 0.34]  # Приближенное распределение (равномерное)

# Демонстрация асимметрии KL-дивергенции
kl_pq = kl_divergence(P, Q)  # KL(P||Q): насколько Q отличается от P
kl_qp = kl_divergence(Q, P)  # KL(Q||P): насколько P отличается от Q

print("KL(P||Q) =", kl_pq)
print("KL(Q||P) =", kl_qp)
print("Асимметрия: KL(P||Q) != KL(Q||P) ->", kl_pq != kl_qp)

print()  # Пустая строка для разделения вывода

# Демонстрация связи между энтропией, кросс-энтропией и KL-дивергенцией
H_P = entropy(P)  # Энтропия истинного распределения
H_PQ = cross_entropy(P, Q)  # Кросс-энтропия между P и Q

# Прямое вычисление KL-дивергенции (для проверки)
KL = np.sum(np.array(P) * np.log(np.array(P) / np.array(Q)))

print("H(P)      =", H_P)  # Энтропия распределения P
print("H(P, Q)   =", H_PQ)  # Кросс-энтропия между P и Q
print("KL(P||Q)  =", KL)  # KL-дивергенция (прямое вычисление)

# Проверка: H(P, Q) = H(P) + KL(P||Q), следовательно KL(P||Q) = H(P, Q) - H(P)
print("Проверка: H(P,Q) - H(P) =", H_PQ - H_P)