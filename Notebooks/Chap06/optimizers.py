"""
Модуль с реализациями различных оптимизаторов для обучения нейронных сетей.
"""
import numpy as np


class SGD:
    """
    Стохастический градиентный спуск (Stochastic Gradient Descent).
    
    Простейший оптимизатор, который обновляет параметры в направлении,
    противоположном градиенту, умноженному на скорость обучения.
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения (learning rate), по умолчанию 1e-2
    """
    
    def __init__(self, params, lr=1e-2):
        """
        Инициализация оптимизатора SGD.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
        """
        self.params = params
        self.lr = lr

    def step(self, grads):
        """
        Выполняет один шаг оптимизации.
        
        Обновляет каждый параметр: param = param - lr * gradient
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        for p, g in zip(self.params, grads):
            p -= self.lr * g


class SGDMomentum:
    """
    SGD с моментумом (Momentum).
    
    Добавляет инерцию к обновлениям параметров, используя экспоненциально
    взвешенное скользящее среднее градиентов. Это помогает сглаживать
    колебания и ускоряет сходимость в направлениях с устойчивым градиентом.
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения, по умолчанию 1e-2
        momentum (float): Коэффициент моментума (обычно 0.9), по умолчанию 0.9
    """
    
    def __init__(self, params, lr=1e-2, momentum=0.9):
        """
        Инициализация оптимизатора SGD с моментумом.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
            momentum (float): Коэффициент моментума
        """
        self.params = params
        self.lr = lr
        self.momentum = momentum
        # Инициализация векторов скорости (velocity) нулями
        self.v = [np.zeros_like(p) for p in params]

    def step(self, grads):
        """
        Выполняет один шаг оптимизации с моментумом.
        
        Обновляет скорость: v = momentum * v + gradient
        Обновляет параметры: param = param - lr * v
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        mu = self.momentum
        for i, (p, g) in enumerate(zip(self.params, grads)):
            # Обновление скорости с учетом моментума
            self.v[i] = mu * self.v[i] + g
            # Обновление параметров с использованием скорости
            p -= self.lr * self.v[i]


class SGDNesterov:
    """
    Nesterov ускоренный градиент (Nesterov Accelerated Gradient, NAG).
    
    Улучшенная версия моментума, которая использует "lookahead" - учитывает
    будущее положение параметров при вычислении градиента. Это позволяет
    более точно корректировать направление обновления.
    
    Реализован в распространённой "практической" форме (как часто делают в фреймворках):
      v = mu*v + g
      w -= lr * (g + mu*v)
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения, по умолчанию 1e-2
        momentum (float): Коэффициент моментума, по умолчанию 0.9
    """

    def __init__(self, params, lr=1e-2, momentum=0.9):
        """
        Инициализация оптимизатора Nesterov.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
            momentum (float): Коэффициент моментума
        """
        self.params = params
        self.lr = lr
        self.momentum = momentum
        # Инициализация векторов скорости нулями
        self.v = [np.zeros_like(p) for p in params]

    def step(self, grads):
        """
        Выполняет один шаг оптимизации Nesterov.
        
        Обновляет скорость: v = momentum * v + gradient
        Обновляет параметры с учетом "lookahead": param = param - lr * (gradient + momentum * v)
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        mu = self.momentum
        for i, (p, g) in enumerate(zip(self.params, grads)):
            # Обновление скорости
            self.v[i] = mu * self.v[i] + g
            # Обновление параметров с Nesterov-коррекцией (lookahead)
            p -= self.lr * (g + mu * self.v[i])


class Adam:
    """
    Адаптивная оценка моментов (Adaptive Moment Estimation, Adam).
    
    Адаптивный оптимизатор, который вычисляет индивидуальные скорости обучения
    для каждого параметра. Использует скользящие средние первого момента (градиенты)
    и второго момента (квадраты градиентов) для адаптации скорости обучения.
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения, по умолчанию 1e-3
        betas (tuple): Коэффициенты для моментов (beta1, beta2), по умолчанию (0.9, 0.999)
        eps (float): Малое значение для численной стабильности, по умолчанию 1e-8
    """
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):
        """
        Инициализация оптимизатора Adam.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
            betas (tuple): Коэффициенты для первого и второго моментов
            eps (float): Малое значение для предотвращения деления на ноль
        """
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps

        # Инициализация скользящих средних первого момента (m) и второго момента (v)
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        # Счётчик итераций для коррекции смещения (bias correction)
        self.t = 0

    def step(self, grads):
        """
        Выполняет один шаг оптимизации Adam.
        
        Обновляет скользящие средние моментов, применяет коррекцию смещения
        и обновляет параметры с адаптивной скоростью обучения.
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        self.t += 1
        b1, b2 = self.beta1, self.beta2

        for i, (p, g) in enumerate(zip(self.params, grads)):
            # Обновление скользящего среднего первого момента (градиенты)
            self.m[i] = b1 * self.m[i] + (1.0 - b1) * g
            # Обновление скользящего среднего второго момента (квадраты градиентов)
            self.v[i] = b2 * self.v[i] + (1.0 - b2) * (g * g)

            # Коррекция смещения (bias correction) для первого и второго моментов
            m_hat = self.m[i] / (1.0 - b1**self.t)
            v_hat = self.v[i] / (1.0 - b2**self.t)

            # Обновление параметров с адаптивной скоростью обучения
            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)


class Nadam:
    """
    Nesterov-ускоренный Adam (Nadam = Adam + Nesterov-подобный "lookahead").
    
    Комбинирует адаптивность Adam с идеей "lookahead" из Nesterov momentum.
    Это позволяет оптимизатору более точно предсказывать будущее положение
    параметров и корректировать направление обновления.
    
    Алгоритм (одна из самых популярных форм):
      m_t = b1*m_{t-1} + (1-b1)*g_t
      v_t = b2*v_{t-1} + (1-b2)*g_t^2
      m_hat = m_t / (1-b1^t)
      v_hat = v_t / (1-b2^t)
      g_hat = g_t / (1-b1^t)
      m_nesterov = b1*m_hat + (1-b1)*g_hat
      w -= lr * m_nesterov / (sqrt(v_hat)+eps)
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения, по умолчанию 1e-3
        betas (tuple): Коэффициенты для моментов (beta1, beta2), по умолчанию (0.9, 0.999)
        eps (float): Малое значение для численной стабильности, по умолчанию 1e-8
    """

    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):
        """
        Инициализация оптимизатора Nadam.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
            betas (tuple): Коэффициенты для первого и второго моментов
            eps (float): Малое значение для предотвращения деления на ноль
        """
        self.params = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps

        # Инициализация скользящих средних первого момента (m) и второго момента (v)
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        # Счётчик итераций для коррекции смещения
        self.t = 0

    def step(self, grads):
        """
        Выполняет один шаг оптимизации Nadam.
        
        Обновляет скользящие средние моментов, применяет коррекцию смещения,
        вычисляет Nesterov-корректированный момент и обновляет параметры.
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        self.t += 1
        b1, b2 = self.beta1, self.beta2

        for i, (p, g) in enumerate(zip(self.params, grads)):
            # Обновление скользящего среднего первого момента
            self.m[i] = b1 * self.m[i] + (1.0 - b1) * g
            # Обновление скользящего среднего второго момента
            self.v[i] = b2 * self.v[i] + (1.0 - b2) * (g * g)

            # Коррекция смещения для моментов и градиента
            m_hat = self.m[i] / (1.0 - b1**self.t)
            v_hat = self.v[i] / (1.0 - b2**self.t)
            g_hat = g / (1.0 - b1**self.t)

            # Вычисление Nesterov-корректированного момента (lookahead)
            m_nesterov = b1 * m_hat + (1.0 - b1) * g_hat
            # Обновление параметров с использованием Nesterov-момента
            p -= self.lr * m_nesterov / (np.sqrt(v_hat) + self.eps)


class RMSProp:
    """
    RMSProp (Root Mean Square Propagation) - адаптивный оптимизатор.
    
    Использует экспоненциально взвешенное скользящее среднее квадратов градиентов
    для адаптации скорости обучения каждого параметра. Это позволяет уменьшать
    скорость обучения для параметров с большими градиентами и увеличивать для
    параметров с малыми градиентами.
    
    RMSProp является предшественником Adam и особенно эффективен для
    нестационарных задач и рекуррентных нейронных сетей.
    
    Параметры:
        params (list): Список массивов параметров для оптимизации
        lr (float): Скорость обучения, по умолчанию 1e-3
        rho (float): Коэффициент сглаживания (decay rate) для скользящего среднего,
                     обычно 0.9 или 0.99, по умолчанию 0.9
        eps (float): Малое значение для численной стабильности, по умолчанию 1e-8
    """
    
    def __init__(self, params, lr=1e-3, rho=0.9, eps=1e-8):
        """
        Инициализация оптимизатора RMSProp.
        
        Параметры:
            params (list): Список массивов параметров
            lr (float): Скорость обучения
            rho (float): Коэффициент сглаживания для скользящего среднего квадратов градиентов
            eps (float): Малое значение для предотвращения деления на ноль
        """
        self.params = params
        self.lr = lr
        self.rho = rho
        self.eps = eps

        # Инициализация скользящих средних квадратов градиентов (EMA от g^2)
        self.v = [np.zeros_like(p) for p in params]

    def step(self, grads):
        """
        Выполняет один шаг оптимизации RMSProp.
        
        Обновляет скользящее среднее квадратов градиентов и применяет
        адаптивную нормализацию к обновлению параметров.
        
        Алгоритм:
            v = rho * v + (1 - rho) * g^2
            param = param - lr * g / (sqrt(v) + eps)
        
        Параметры:
            grads (list): Список градиентов, соответствующий списку параметров
        """
        rho = self.rho
        for i, (p, g) in enumerate(zip(self.params, grads)):
            # Обновление скользящего среднего квадратов градиентов (экспоненциально взвешенное)
            self.v[i] = rho * self.v[i] + (1.0 - rho) * (g * g)
            # Обновление параметров с адаптивной нормализацией
            # Деление на sqrt(v) адаптирует скорость обучения для каждого параметра
            p -= self.lr * g / (np.sqrt(self.v[i]) + self.eps)