Рассмотрим логистическую регрессию с двумя параметрами $\phi_0,\phi_1$ и бинарной кросс-энтропией (BCE) на выборке $\{(x_i,y_i)\}_{i=1}^m$, где $y_i\in\{0,1\}$.

## Модель и функция потерь (BCE)

Линейный скор:
$$
z_i = \phi_0 + \phi_1 x_i,
$$
вероятность класса $1$:
$$
p_i = \sigma(z_i)=\frac{1}{1+e^{-z_i}}.
$$

Средняя BCE:
$$
J(\phi_0,\phi_1)=\frac{1}{m}\sum_{i=1}^m\Big(-y_i\ln p_i-(1-y_i)\ln(1-p_i)\Big).
$$

---

## Градиент и Гессиан

### Градиент
Из стандартных вычислений для логистической регрессии:
$$
\frac{\partial J}{\partial \phi_0}=\frac{1}{m}\sum_{i=1}^m (p_i-y_i),\qquad
\frac{\partial J}{\partial \phi_1}=\frac{1}{m}\sum_{i=1}^m (p_i-y_i)x_i.
$$

### Гессиан (алгебраическая форма)
Ключевой факт:
$$
\frac{d p_i}{d z_i} = p_i(1-p_i).
$$
Обозначим веса
$$
w_i = p_i(1-p_i).
$$
Для любых конечных $\phi_0,\phi_1$ имеем $p_i\in(0,1)$, значит
$$
w_i \in (0,1/4].
$$

Тогда матрица Гессе по $(\phi_0,\phi_1)$:
$$
H[J](\phi_0,\phi_1)=
\begin{pmatrix}
\frac{\partial^2 J}{\partial \phi_0^2} & \frac{\partial^2 J}{\partial \phi_0\partial \phi_1}\\
\frac{\partial^2 J}{\partial \phi_1\partial \phi_0} & \frac{\partial^2 J}{\partial \phi_1^2}
\end{pmatrix}
=
\frac{1}{m}\sum_{i=1}^m w_i
\begin{pmatrix}
1 & x_i\\
x_i & x_i^2
\end{pmatrix}.
$$

В развернутом виде:
$$
H[J]=\frac{1}{m}
\begin{pmatrix}
\sum_{i=1}^m w_i & \sum_{i=1}^m w_i x_i\\
\sum_{i=1}^m w_i x_i & \sum_{i=1}^m w_i x_i^2
\end{pmatrix}.
$$

---

## Выпуклость: собственные значения неотрицательны через $\mathrm{tr}$ и $\det$

Поскольку $H[J]$ — симметричная матрица $2\times 2$, её собственные значения $\lambda_1,\lambda_2$ удовлетворяют:
$$
\lambda_1+\lambda_2=\mathrm{tr}(H[J]),\qquad \lambda_1\lambda_2=\det(H[J]).
$$
Если $\mathrm{tr}(H[J])>0$ и $\det(H[J])\ge 0$, то $\lambda_1,\lambda_2\ge 0$ (а значит $H[J]\succeq 0$ и $J$ выпукла).

### 1) След положителен
$$
\mathrm{tr}(H[J])=\frac{1}{m}\left(\sum_{i=1}^m w_i+\sum_{i=1}^m w_i x_i^2\right)
=\frac{1}{m}\sum_{i=1}^m w_i(1+x_i^2).
$$
Так как для конечных параметров $w_i>0$ и $1+x_i^2>0$, получаем:
$$
\mathrm{tr}(H[J])>0.
$$

### 2) Определитель неотрицателен
Обозначим:
$$
A=\sum w_i,\quad B=\sum w_i x_i,\quad C=\sum w_i x_i^2.
$$
Тогда
$$
\det(H[J])=\frac{1}{m^2}(AC-B^2).
$$
Неравенство Коши–Буняковского в “взвешенной” форме даёт:
$$
\left(\sum_{i=1}^m w_i x_i\right)^2 \le \left(\sum_{i=1}^m w_i\right)\left(\sum_{i=1}^m w_i x_i^2\right),
$$
то есть
$$
B^2 \le AC \;\Rightarrow\; AC-B^2\ge 0 \;\Rightarrow\; \det(H[J])\ge 0.
$$

Следовательно, для всех конечных $\phi$ матрица Гессе $H[J]$ положительно полуопределена, а функция $J(\phi_0,\phi_1)$ **выпукла**.

---

## Когда выпуклость строгая (собственные значения строго положительные)
$\det(H[J])>0$ эквивалентно $AC-B^2>0$, что эквивалентно положительной **взвешенной дисперсии** признака $x$:
$$
AC-B^2 = \left(\sum w_i\right)^2 \mathrm{Var}_w(x),
$$
где
$$
\mathrm{Var}_w(x)=\frac{\sum w_i(x_i-\mu_w)^2}{\sum w_i},\qquad
\mu_w=\frac{\sum w_i x_i}{\sum w_i}.
$$
Так как при конечных $\phi$ все $w_i>0$, то $\mathrm{Var}_w(x)>0$ тогда и только тогда, когда **не все $x_i$ одинаковы**. В этом случае $H[J]\succ 0$ для всех конечных $\phi$, и $J$ строго выпукла по $(\phi_0,\phi_1)$.

<details>
<summary>Важная ремарка про “уникальный минимум” в логистической регрессии</summary>

Даже при выпуклости (и даже при строгой выпуклости для конечных $\phi$) может не существовать конечного минимума, если данные линейно разделимы: тогда $J(\phi)$ стремится к $0$ при $\|\phi\|\to\infty$ вдоль разделяющего направления, но минимум не достигается. Это не противоречит выпуклости: функция просто не имеет точки минимума в $\mathbb{R}^2$.

</details>

Если хотите, могу для конкретного набора $\{x_i\}$ показать явно, при каких условиях $\det(H[J])>0$ (и значит оба собственных значения $>0$) и как это связано с “полным рангом” матрицы признаков в 2D.